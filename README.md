# Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension


[**Tasks**](#task-description) | [**Dataset**](#dataset) | [**Checkpoints**](#Checkpoints) |
[**Paper**](https://arxiv.org/pdf/2302.13619) |
[**Citation**](#citation) | [**License**](#license)

This repository contains resources for accessing the official benchmarks, codes and checkpoints of the paper:  [***Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension***](https://arxiv.org/pdf/2302.13619).

The paper was accepted by EMNLP 2023 and GenBench Workshop! ğŸ‰

We collect $\textbf{Orca}$, the first few-shot Chinese CMRC benchmark. Concretely, we  annotate the following components in Orca: 1) $\textbf{Topic}$, which consists of several sentences to drive the whole conversation; 2) $\textbf{Domain}$, only one or two words indicate the specific field o which the content of the conversation belongs; 3) $\textbf{Conversations}$, where each turn is assigned a golden knowledgeable passage. Importantly, the question and response in each turn  are human-crafted, leading to more coherent conversations. And response-related passages are also manually selected from the search engine. In total, we collect manually annotated 831 conversations and 4742 turns in Orca. Even though the size of our dataset could be questioned, our goal is to build strong CMRC models under few-shot or even zero-shot settings, and without needing to collect data for each target domain.

The highlight of $\textbf{Orca}$ are three-folds: 1) Each turn of a conversation is assigned a ground-truth knowledgeable passage, fitting human cognition towards CMRC. 2) Responses are free-form text with demonstrative pronouns instead of extracted answer span from the passage, making the expression more natural. 3) As the conversations are collected based on hot-topics from social media across 33 domain, the domains of conversations are much more diverse than existing datasets.

## DataSet

Orca has the following salient features: 1) Data in Orca are collected from November 2021 to November 2022 on Weibo, one of the most popular social media platforms in China. This means that the collected data reflect real human interests, are quite new, and have never been included in earlier benchmarks, posing a challenge to existing language models.  Moreover, good results on
Orca are of practical interest. 2) We carefully  annotate conversations across 33 domains. In contrast, as the current commonly-used datasets, data in CoQA are only from 7 domains, DoQA contains data from 3 domains. The variety of data domains makes Orca closer to real scenarios and  better-evaluating the generalization of CMRC models. 3) Answers at each turn in a conversation are  natural and informative responses from human annotation rather than certain spans in the provided passage. This way, we can both evaluate models' comprehension ability and generation ability.

$\textbf{Orca}$ contains 831 conversations and 4,742 turns. For each conversation, there are 5.71 turns on average, and each turn is assigned a knowledgeable passage. We randomly split collected data into support (train) and test set. Concretely, the support set and test set contain 200 and 631 conversations, separately.

Concretely, we store our dataset in json files:


```
 "0": {
        "topic": "é‚“ä¼¦èµ„æœ¬ç‰ˆå›¾",
        "domain": "äººç‰©",
        "context": {
            "0": {
                "query": "é‚“è®ºæ˜¯è°ï¼Ÿ",
                "response": "é‚“ä¼¦ï¼Œ1992å¹´10æœˆ21æ—¥å‡ºç”Ÿäºæ²³åŒ—çœçŸ³å®¶åº„å¸‚ï¼Œä¸­å›½å†…åœ°å½±è§†ç”·æ¼”å‘˜ï¼Œæ¯•ä¸šäºä¸Šæµ·æˆå‰§å­¦é™¢è¡¨æ¼”ç³»ã€‚",
                "query-type": "Causal",
                "passage": "é‚“ä¼¦ï¼Œ1992å¹´10æœˆ21æ—¥å‡ºç”Ÿäºæ²³åŒ—çœçŸ³å®¶åº„å¸‚ï¼Œä¸­å›½å†…åœ°å½±è§†ç”·æ¼”å‘˜ï¼Œæ¯•ä¸šäºä¸Šæµ·æˆå‰§å­¦é™¢è¡¨æ¼”ç³»ã€‚å¤©çœ¼æŸ¥Appæ˜¾ç¤ºï¼Œé‚“ä¼¦åä¸‹æœ‰2å®¶å…¬å¸ï¼Œåˆ†åˆ«ä¸ºé‚“ä¼¦ï¼ˆä¸Šæµ·ï¼‰å½±è§†æ–‡åŒ–å·¥ä½œå®¤å’ŒèˆŸå±±é‚“ä¼¦å½±è§†æ–‡åŒ–å·¥ä½œå®¤ï¼Œ2å®¶å·¥ä½œå®¤å‡ä¸ºé‚“ä¼¦ä¸ªäººç‹¬èµ„ä¼ä¸šï¼Œåˆ†åˆ«æˆç«‹äº2016å¹´å’Œ2018å¹´ã€‚"
            },
...
            "4": {
                "query": "ä»–æœ‰å“ªäº›ä»£è¨€",
                "response": "é‚“ä¼¦æœ‰å®æ ¼ä¸½ã€é›ªèŠ±ç§€ã€æ¬§è±é›…ã€è”åˆåˆ©åç­‰ä»£è¨€",
                "query-type": "List",
                "passage": "é‚“ä¼¦ä»£è¨€æ±‡æ€»ï¼š 1ã€å®æ ¼ä¸½ï¼šå“ç‰Œä»£è¨€äºº 2ã€é›ªèŠ±ç§€ï¼šå“ç‰Œäºšå¤ªåŒºä»£è¨€äºº 3ã€æ¬§è±é›…ï¼šå½©å¦†å“ç‰Œä»£è¨€äºº 4ã€è”åˆåˆ©åï¼šä¸­å›½åŒºæ´—æŠ¤å‘ä»£è¨€äººï¼Œæ¸…æ‰¬å‡€çˆ½ä»£è¨€äºº"
            }
        }
    },
```

Each conversation in the dataset contains a unique number, `Turn_no`, `Topic`, `domain`  unique within a conversation, the  `Question`, `query`, `response`, `query-type` with `passage` unique within each turn.

### Download

- **Support Set**
  - [**5-shot Set**](https://hkustgz-my.sharepoint.com/:u:/g/personal/nchen022_connect_hkust-gz_edu_cn/EQSQOzgua51Omi8j-y6V7j4BOOFoIMYXg-Vg4BFNLuyKCw?e=gqFe6U)
  - [**10-shot Set**](https://hkustgz-my.sharepoint.com/:u:/g/personal/nchen022_connect_hkust-gz_edu_cn/EfUGD87nc79BgngLJkPKmEoB9OhI4vSckrQhiwg3YZ7dIQ?e=fesXnX)
  - [**200-shot Set**](https://hkustgz-my.sharepoint.com/:u:/g/personal/nchen022_connect_hkust-gz_edu_cn/EXXUL8WIpBVMo5PGe-Mef0ABK4wDLhLBCYeRJ81hr_-ggA?e=dTZDiZ)
  
- [**Test Set**]
For the test set, please contact the chennuo26@gmail.com.



## Checkpoints

### Results
Here we report automatic and huaman evaluations results of four baselines in our paper.

![](shiyan.png) 

###  Download

|Model |  zero-shot| 5-shot | 10-shot | 200-shot |
| :----- | :-------------------:| :------------------: | :------------------: |:------------------: |
| T5 | T5-base| T5-base |  T5-base |  T5-base | 
| BART | [BART-Large](https://hkustgz-my.sharepoint.com/:u:/g/personal/nchen022_connect_hkust-gz_edu_cn/EfQYlzgOAI9Bogfy2f2hSCwBu_z4acFeFY16jTz5G2I8eg?e=LBQ1pZ)    |[BART-Large](https://hkustgz-my.sharepoint.com/:f:/g/personal/nchen022_connect_hkust-gz_edu_cn/EmA0EfSvS85KtYXferY0MjIBoiojfGMRDZxBs8KbruY6VQ?e=uSdZrQ)  | [BART-Large](https://hkustgz-my.sharepoint.com/:u:/g/personal/nchen022_connect_hkust-gz_edu_cn/EemWOJFzZy1JmmWhm3vCJmQBpyOJfGLbG7o-VSC6Ord16A?e=lANCbq)  | [BART-Large](https://hkustgz-my.sharepoint.com/:f:/g/personal/nchen022_connect_hkust-gz_edu_cn/EpFR5MDb-zZKnPtAQNzmvaEBvrVNGrjPMfszJT8hcTCGdw?e=QeQPWr)  |


## Evaluation

### inference 
We provide the inference code of BART, please refer to utils/inference_bart.py.
```
python3 utils/inference_bart.py --type --model_path --output_path --bsz
```
  Then it can could generate two files: ***labels.txt*** and ***prediction.txt***
  
  Of note, to ease inference, we reformulate the test set to run this transcript. Please contact the chennuo26@gmail.com for the corresponding file.
  
### Compute metrics
```
python3 utils/evaluate_metrics.py --predict_path -labels_path
```
Running this script could lead to computing the automatic metrics of the model.



## Citation

```
@inproceedings{Chen2023NaturalRG,
  title={Natural Response Generation for Chinese Reading Comprehension},
  author={Nuo Chen and Hongguang Li and Yinan Bao and Baoyuan Wang and Jia Li},
  year={2023}
}
@article{Chen2023OrcaAF,
  title={Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension},
  author={Nuo Chen and Hongguang Li and Yinan Bao and Junqing He and Xinshi Lin and Qi Yang and Jianfeng Liu and Ruyi Gan and Jiaxing Zhang and Baoyuan Wang and Jia Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13619}
}
```
